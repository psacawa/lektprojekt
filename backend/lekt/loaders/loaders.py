from collections import namedtuple
from django.db import IntegrityError
from math import sqrt
from os.path import join
from progress.bar import Bar
from spacy.tokens.doc import Doc
import logging
import pandas as pd
import spacy
import sys
import gc
import json
from django.conf import settings
from django.db import transaction
from .language import LanguageParser
from lekt import models


logger: logging.Logger = logging.getLogger(__name__)
logger.setLevel(logging.CRITICAL)

ValidationData = namedtuple("ValidationData", ["length", "propriety"])


class SpacyCorpusLoader(object):
    """ 
    This class serves to load a specfic corpus, as represented by a flat csv file
    and performs the NLP to enter it into the database. Some decoupling possible to
    separate ETL. It may be necessary later to refactor to inheritance strategy a la
    Django views.
    Notions of 'base', 'target' within the variable names of this models are strictly
    a matter of convention. Their is no intrinsic directionality to the parallel 
    corpora used as input.
    Intended use:

    migrations.RunPython (
        SpacyCorpusLoader (
            *args
        )
    ) 
    """

    def __init__(self, parallel_corpus, **kwargs):
        """
        `parallel_corpus` must be a flat file of examples to read from. This file must 
        have indices and column names. Constructor will verify that base/target `lid`s  
        correspond to the column name of the first/second columns respectively.

        :parallel_corpus: e.g. sd_examples.csv
        :base_lid: e.g. 'en'
        :base_model: e.g. en_core_web_md
        :target_lid: e.g. 'en'
        :target_model: e.g. 'es_core_news_md'
        """
        self.parallel_corpus = parallel_corpus

        Base = kwargs.pop("base", lambda: None)
        Target = kwargs.pop("target", lambda: None)
        self.base_loader = Base()
        self.target_loader = Target()

        self.base_lid = self.base_loader.lid
        self.target_lid = self.target_loader.lid

        try:
            self.df: pd.DataFrame = pd.read_csv(self.parallel_corpus, index_col=0)
        except Exception as e:
            logger.error(f"Unable to read SD data from {self.parallel_corpus}.")
            raise

    def __call__(self, limit=None):
        """
        This is the entrypoint to the migration as called by RunPython
        """
        if isinstance(limit, int):
            df = self.df[:limit]
        else:
            df = self.df

        progress: Bar = Bar(f"Processing parallel corpus", max=len(df))
        for i, s in df.iterrows():
            self.process_pair(s)
            progress.next()
        progress.finish()

        #  self.base_loader.compute_occurences()
        #  self.target_loader.compute_occurences()

    def process_pair(self, pair: pd.Series):
        """ Process an example pair, represented in the format of a pandas Series """
        base_raw = pair[self.base_lid]
        target_raw = pair[self.target_lid]

        try:
            base_phrase = self.base_loader.process_phrase(base_raw)
            target_phrase = self.target_loader.process_phrase(target_raw)
        except IntegrityError as e:
            # TODO: this should complain when a oari is repeated, but how would the lang
            #  loaders in principle know about that
            #  ipdb.set_trace ()
            logger.error(f"{e}")
            return

        active = self.valid_phrase_pair(base_phrase, target_phrase)

        forward_pair = models.PhrasePair.objects.create(
            base=base_phrase, target=target_phrase, source="SD", active=active
        )
        backward_pair = models.PhrasePair.objects.create(
            base=target_phrase, target=base_phrase, source="SD", active=active
        )

    def valid_phrase_pair(self, base_phrase, target_phrase) -> bool:
        """
        Validates a phrase pair for suitability for active inclusion in the database.
        Invalid pairs are still included, but with 'active' attribute False.
        """
        #  base_data = self.get_validation_data(self.base_nlp(base_phrase.text))
        #  target_data = self.get_validation_data(self.target_nlp(target_phrase.text))
        base_data = base_phrase.validation_data
        target_data = target_phrase.validation_data
        l2_prop = sqrt(base_data.propriety ** 2 + target_data.propriety ** 2)
        avg_len = (base_data.length + target_data.length) / 2

        #  arbitrary criterion
        valid = l2_prop < 0.4 and avg_len >= 4
        if not valid:
            logger.debug(
                f"Found invalid phrase pair: {base_phrase.text} {target_phrase.text}"
            )

        return valid


class PollyLoader(object):
    """
    import languages and accents from voices.json
    which is the output generated by:

    > aws --output json polly describe-voices
    """

    VOICE_DATA_SOURCE = join(settings.ASSET_DIR, "voices.json")
    assigned_voices = {("en", "Joanna"), ("es", "Lucia")}

    def __init__(self, apps):

        try:
            with open(self.VOICE_DATA_SOURCE) as voices_file:
                self.voices_json = json.loads(voices_file.read())
        except Exception as e:
            logger.error(f"Failure to read from {self.VOICE_DATA_SOURCE}")
            raise

        self.new_languages = []
        self.new_voices = []

        logger.info(f"Obtaining historal models Language, Voice.")
        self.Language = apps.get_model("lekt", "Language")
        self.Voice = apps.get_model("lekt", "Voice")

    def __call__(self):
        self.import_polly()
        self.assign_default_voices()

    def import_polly(self):

        progress = Bar("Importing voices", max=len(self.voices_json["Voices"]))
        for voice in self.voices_json["Voices"]:
            voice_name = voice["Id"]
            gender = voice["Gender"][0]
            aid = voice["LanguageCode"]
            accent_name: str = voice["LanguageName"]
            if len(accent_name.split()) == 1:
                base_lang_name = accent_name
            else:
                base_lang_name = accent_name.split()[1]
            lid = aid[:2]
            logger.debug(
                f"Processing {voice_name} ({gender}) who is {accent_name} ({aid}), "
                f"a dialect of {base_lang_name} ({lid})."
            )

            lang_query = self.Language.objects.filter(lid=lid)
            if lang_query.count() == 0:
                cur_lang = self.Language(name=base_lang_name, lid=lid)
                logger.debug(f"new language found: {cur_lang}")
                self.new_languages.append(cur_lang)
                cur_lang.save()
            else:
                cur_lang = lang_query[0]

            cur_voice = self.Voice(
                lang=cur_lang,
                name=voice_name,
                accent=accent_name,
                aid=aid,
                gender=gender,
            )
            self.new_voices.append(cur_voice)
            cur_voice.save()
            progress.next()
        progress.finish()

    def assign_default_voices(self):
        """docstring for assign_default_voices"""

        for lang in self.Language.objects.all():
            if lang.lid in self.assigned_voices:
                try:
                    voice = self.Voice.objects.get(name=self.assigned_voices[lang.lid])
                except KeyError as e:
                    logger.error(
                        f"Voice {self.assigned_voices[lang.lid]}"
                        "for {lang} not found in database."
                    )
            else:
                try:
                    voice = lang.voice_set.order_by("id")[0]
                except Exception as e:
                    logger.error(f"{__module__}: No voices attached to {lang}.")
                    raise e
            lang.default_voice = voice
            logger.debug(f"Setting {lang.lid} default voice to {voice.name}")
            lang.save()
